{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cfd969f0-d9ef-48da-bad8-cc775f44bc04",
   "metadata": {},
   "source": [
    "# –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–π –æ–∫—Ä–∞—Å–∫–∏ –∫–Ω–∏–≥"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2066aa00-799c-4778-954a-8f46d380d971",
   "metadata": {},
   "source": [
    "–í –¥–∞–Ω–Ω–æ–º –Ω–æ—É—Ç–±—É–∫–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω –∞–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–Ω–∏–≥–∏ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Å–ª–æ–≤–∞—Ä—è [Hedonometer](https://en.wikipedia.org/wiki/Hedonometer). –¶–µ–ª—å –ø—Ä–æ–µ–∫—Ç–∞ ‚Äî –≤–∏–∑—É–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –¥–∏–Ω–∞–º–∏–∫—É —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω–æ–≥–æ —Ñ–æ–Ω–∞ —Ç–µ–∫—Å—Ç–∞,‚ÄØ–æ—Ç—Å–ª–µ–∂–∏–≤–∞—è ¬´–Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏–µ¬ª —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤ –ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è. –¢–∞–∫–æ–π –ø–æ–¥—Ö–æ–¥ –ø–æ–∑–≤–æ–ª—è–µ—Ç –≤—ã—è–≤–∏—Ç—å –∫–ª—é—á–µ–≤—ã–µ —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–µ –º–æ–º–µ–Ω—Ç—ã —Å—é–∂–µ—Ç–∞, —É–≤–∏–¥–µ—Ç—å –ø–∏–∫–∏ –∏ —Å–ø–∞–¥—ã –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è, –∞ —Ç–∞–∫–∂–µ —Å–¥–µ–ª–∞—Ç—å –∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã–µ –≤—ã–≤–æ–¥—ã –æ –ø—Å–∏—Ö–æ–ª–æ–≥–∏—á–µ—Å–∫–æ–π –¥–∏–Ω–∞–º–∏–∫–µ –ø–µ—Ä—Å–æ–Ω–∞–∂–µ–π –∏ —Å–æ–±—ã—Ç–∏–π.\n",
    "\n",
    "–ü–æ—á–µ–º—É –≤—ã–±—Ä–∞–Ω Hedonometer, –∞ –Ω–µ —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä—ã\n",
    "\n",
    "- –í—ã—Å–æ–∫–∞—è —Å–∫–æ—Ä–æ—Å—Ç—å. Lexicon-based –ø–æ–¥—Ö–æ–¥ –Ω–µ —Ç—Ä–µ–±—É–µ—Ç –≤—ã—á–∏—Å–ª–∏—Ç–µ–ª—å–Ω—ã—Ö –º–æ—â–Ω–æ—Å—Ç–µ–π GPU –∏ —Ä–∞–±–æ—Ç–∞–µ—Ç –≤ —Ä–∞–∑—ã –±—ã—Å—Ç—Ä–µ–µ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —Ç—Ä–∞–Ω—Å—Ñ–æ—Ä–º–µ—Ä–∞–º–∏ –∏–ª–∏ –¥—Ä—É–≥–∏–º–∏ –Ω–µ–π—Ä–æ—Å–µ—Ç–µ–≤—ã–º–∏ –º–æ–¥–µ–ª—è–º–∏.\n",
    "\n",
    "- –°–ª–æ–∂–Ω–æ—Å—Ç—å –≤–Ω–µ–¥—Ä–µ–Ω–∏—è –º–∏–Ω–∏–º–∞–ª—å–Ω–∞. –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –ø—Ä–µ–¥—Ä–∞—Å—Å—á–∏—Ç–∞–Ω–Ω—ã–π csv —Ñ–∞–π–ª."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76a96b5d-7836-4716-9b9d-c4a871a5a7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "from razdel import sentenize\n",
    "import re\n",
    "import dash\n",
    "from dash import dcc, html, Input, Output\n",
    "import plotly.graph_objects as go"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb45a1b-2dea-4529-b677-053c3f4c5fc3",
   "metadata": {},
   "source": [
    "## Hedonometer –º–æ–¥–µ–ª—å"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da7d2c6-1a4e-4f6a-ac70-d4b711ad952e",
   "metadata": {},
   "source": [
    "–ó–∞–≥—Ä—É–∑–∏–º –∏ –æ–±—ä–µ–¥–∏–Ω–∏–º —Ä—É—Å—Å–∫–∏–π –∏ –∞–Ω–≥–ª–∏–π—Å–∫–∏–π —Å–ª–æ–≤–∞—Ä—å Hedonometer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8e628580-be5e-4a50-9f9e-9fbb24a789e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19455"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ru_words = pd.read_csv(\"Hedonometer_ru.csv\", index_col=0)\n",
    "en_words = pd.read_csv(\"Hedonometer_en.csv\", index_col=0)\n",
    "words_dict = dict(tuple(zip(ru_words['Word'], ru_words['Happiness Score'])) + tuple(zip(en_words['Word'], en_words['Happiness Score'])))\n",
    "len(words_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b371119-6454-42d4-abae-4ed92dc59766",
   "metadata": {},
   "source": [
    "–£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–æ–ø —Å–ª–æ–≤ –Ω–µ –∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è, —Ç.–∫. –¥–∞–∂–µ –Ω–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ —Å–ª–æ–≤–∞ –º–µ–Ω—è—é—Ç —ç–º–æ—Ü–∏–æ–Ω–∞–ª—å–Ω—ã–π –æ—Ç—Ç–µ–Ω–æ–∫ —Ç–µ–∫—Å—Ç–∞, —É—Ä–∞–≤–Ω–æ–≤–µ—à–∏–≤–∞—é—Ç –æ–∫—Ä–∞—à–µ–Ω–Ω—ã–µ —Å–ª–æ–≤–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "38ce8990-5981-452f-bd34-55980f585c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_sentiment(text):\n",
    "    cleaned = re.sub(r'[^A-Za-z–ê-–Ø–∞-—è–Å—ë]', ' ', text).lower()\n",
    "    tokens = cleaned.split()\n",
    "    words = [word for word in tokens if word in words_dict]\n",
    "    if not words:\n",
    "        return 5.\n",
    "    word_freq = Counter(words)\n",
    "    total_score = sum(words_dict[word] * freq for word, freq in word_freq.items())\n",
    "    total_count = sum(word_freq.values())\n",
    "    return total_score / total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ce4672-a924-494a-80ae-96633f582567",
   "metadata": {},
   "source": [
    "## –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–æ–±—Ä–∞–±–æ—Ç–∫–∞ –∫–Ω–∏–≥–∏. –î–µ–ª–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8eb5dc1b-f02d-4edb-b534-4ce5bd5785d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"hobbit.txt\", \"r\") as f:\n",
    "    text = f.read().strip().replace(\"\\xa0\", \" \").replace(\"‚Ä¶\", \"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63d38654-723a-4116-9965-f2640daabab6",
   "metadata": {},
   "source": [
    "–¢–µ–∫—Å—Ç –¥–µ–ª–∏—Ç—Å—è –Ω–∞ —á–∞–Ω–∫–∏ —Å —Ä–∞–≤–Ω—ã–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ–º –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2147ee07-800e-4654-8ba8-ef14d21fbae7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'–ñ–∏–ª-–±—ã–ª –≤ –Ω–æ—Ä–µ –ø–æ–¥ –∑–µ–º–ª–µ–π —Ö–æ–±–±–∏—Ç. –ù–µ –≤ –∫–∞–∫–æ–π-—Ç–æ —Ç–∞–º –º–µ—Ä–∑–∫–æ–π –≥—Ä—è–∑–Ω–æ–π —Å—ã—Ä–æ–π –Ω–æ—Ä–µ, –≥–¥–µ —Å–æ –≤—Å–µ—Ö —Å—Ç–æ—Ä–æ–Ω —Ç–æ—Ä—á–∞—Ç —Ö–≤–æ—Å—Ç—ã —á–µ—Ä–≤–µ–π –∏ –ø—Ä–æ—Ç–∏–≤–Ω–æ –ø–∞—Ö–Ω–µ—Ç –ø–ª–µ—Å–µ–Ω—å—é, –Ω–æ –∏ –Ω–µ –≤ —Å—É—Ö–æ–π –ø–µ—Å—á–∞–Ω–æ–π –≥–æ–ª–æ–π –Ω–æ—Ä–µ, –≥–¥–µ –Ω–µ –Ω–∞ —á—Ç–æ —Å'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "from tqdm import tqdm\n",
    "\n",
    "LENGTH=20\n",
    "\n",
    "def split_text(text: str, n: int):\n",
    "    sentences = [sent.text for sent in sentenize(text)]\n",
    "\n",
    "    chunks = []\n",
    "    for i in range(0, len(sentences), n):\n",
    "        chunk = ' '.join(sentences[i:i + n])\n",
    "        chunks.append(chunk)\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "chunks = split_text(text, LENGTH)\n",
    "chunks[0][:200]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2807113f-6936-4f17-8b47-12ba20a4cd6f",
   "metadata": {},
   "source": [
    "## –ü–æ—Å—Ç—Ä–æ–µ–Ω–∏–µ –≥—Ä–∞—Ñ–∏–∫–∞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "34e409f8-c6c6-4a60-b8ed-159a3fb22e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = [calculate_sentiment(i) for i in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0947c725-dfa2-45cd-8a3f-33d43aaf439d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def moving_average(data, window_size):\n",
    "    series = pd.Series(data)\n",
    "    rolling_avg = series.rolling(window=window_size, center=True, min_periods=1).mean()\n",
    "    return rolling_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "204b1b5e-ac16-48bc-a2b9-fe8ab8b19a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "WINDOW_SIZE = 5\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    \"index\": list(range(len(chunks))),\n",
    "    \"score\": scores,\n",
    "    \"smooth_score\": moving_average(scores, WINDOW_SIZE),\n",
    "    \"text\": chunks\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c06a45bb-1521-4326-90e1-0a2cd85847bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"650\"\n",
       "            src=\"http://127.0.0.1:8050/\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7798dec06120>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = go.Figure()\n",
    "# –°—ã—Ä–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['index'], y=df['score'],\n",
    "    mode='markers', name='Raw Score',\n",
    "    marker=dict(size=5, color='aquamarine')\n",
    "))\n",
    "# –£—Å—Ä–µ–¥–Ω—ë–Ω–Ω–∞—è –ª–∏–Ω–∏—è\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=df['index'], y=df['smooth_score'],\n",
    "    mode='lines', name=f'Rolling Mean (win={11})',\n",
    "    line=dict(color='blue', dash='dash'),\n",
    "))\n",
    "\n",
    "fig.update_layout(\n",
    "    title='Sentiment Analysis: Raw vs Rolling Average',\n",
    "    xaxis_title='Fragment Index',\n",
    "    yaxis_title='Sentiment Score',\n",
    "    hovermode='x unified'\n",
    ")\n",
    "\n",
    "# Dash-–ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ\n",
    "app = dash.Dash(__name__)\n",
    "app.title = \"Sentiment Book Viewer\"\n",
    "app.layout = html.Div([\n",
    "    html.H1(\"üìñ –ê–Ω–∞–ª–∏–∑ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–∏—è –∫–Ω–∏–≥–∏\", style={'textAlign': 'center'}),\n",
    "    dcc.Graph(id='sentiment-graph', figure=fig),\n",
    "    html.H3(\"–í—ã–±—Ä–∞–Ω–Ω—ã–π —Ñ—Ä–∞–≥–º–µ–Ω—Ç:\"),\n",
    "    html.Div(id='text-output', style={\n",
    "        'whiteSpace': 'pre-wrap',\n",
    "        'border': '1px solid #ccc',\n",
    "        'padding': '1em',\n",
    "        'backgroundColor': '#f9f9f9'\n",
    "    })\n",
    "])\n",
    "\n",
    "@app.callback(\n",
    "    Output('text-output', 'children'),\n",
    "    Input('sentiment-graph', 'clickData')\n",
    ")\n",
    "def display_text(clickData):\n",
    "    if clickData is None:\n",
    "        return \"–ö–ª–∏–∫–Ω–∏ –Ω–∞ —Ç–æ—á–∫—É, —á—Ç–æ–±—ã —É–≤–∏–¥–µ—Ç—å —Ñ—Ä–∞–≥–º–µ–Ω—Ç —Ç–µ–∫—Å—Ç–∞.\"\n",
    "    idx = clickData['points'][0]['x']\n",
    "    return chunks[int(idx)]\n",
    "\n",
    "\n",
    "app.run(debug=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
